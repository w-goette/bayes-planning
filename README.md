# Sample Size Planning for Bayesian Equivalence Tests

Repository containing the R scripts and related documentation necessary to perform a Bayes factor design analysis (BFDA) as outlined by [Schonbrodt and Wagenmakers (2018)](https://link.springer.com/content/pdf/10.3758/s13423-017-1230-y.pdf). Methods in this repository are specific to a fixed-n design testing the hypothesis of equivalence between two (or more) groups, though they could be used for sample size planning of more traditional hypotheses of difference if needed.

## General Information

R scripts are grouped by the general functions they define. Documentation of each function, their arguments, and general notes on use considerations are provided in the scripts as well. A helper script ("00 - ...") is also provided that can be run to automatically set-up a new R session with the needed functions and summary statistics corresponding to the cognitively normal sample of [Cullum et al. (2014)](https://doi.org/10.1017/S1355617714000873). The helper script assumes that all of the R files within the same folder of this repository have been downloaded and are saved in the same working directory as the open R session. Alternative to this, the script can be modified so that the file path correctly points to wherever the R files have been saved: e.g., source("C:/Users/...user name.../Downloads/01 - distributions.R").

## Simulation of Discrete, Bounded Raw Scores

In order to maximize the robustness and generalizability of the simulation methods, users must specify one of three distributions for which the simulated raw scores will follow: the beta-binomial, negative binomial, or Poisson. These distributions were chosen for their general applicability to the kinds of data that neuropsychological research typically records, namely sum scores or time measured in whole seconds. Simulating data as continuous (e.g., from a normal/Gaussian distribution) can introduce a variety of biases that could make estimation of sample sizes overly optimistic. For example, it would make no sense to simulate the possibility of a person scoring a 32.09381 on the MMSE since observed scores can only be integer values and must be between 0 and 30. Additionally, most standard simulation distributions are strictly symmetric, which can be strongly misleading when considering tests with well known skew (e.g., the BNT and MMSE). By simulating all observed scores to be bounded (at 0 for all distribution and at some upper limit for the beta-binomial) and discrete (i.e., taking only integer values), it is possible to check sample size needs under appropriately pessimistic conditions. For example, range restriction from ceiling (or floor) effects is known to attenuate standardized effect size estimates (e.g., [Bobko et al., 2001](https://doi.org/10.1177/109442810141003); [Hunter et al., 2006](https://doi.org/10.1037/0021-9010.91.3.594); [Simkovic & Trauble, 2019](https://doi.org/10.1371%2Fjournal.pone.0220889)), so being able to readily simulate such potential effects can help to clarify true sample size needs when power and/or error rates may be impacted by data distributions.

The specification of the beta-binomial, negative binomial, and Poisson distribution is hopefully largely unobtrusive. Users need only indicate the desired distribution ("bbin", "nbin", or "pois", respectively) along with the desired raw score mean (all distributions), standard deviation ("bbin" and "nbin"), and test length/number of items/size ("bbin"). Internal functions then scale this information appropriately to produce sample test data with the desired overall characteristics. Importantly, users do not need to worry about specifying effect sizes on the raw scale either; instead, these effect sizes are specified on the standardized scale and then scaled internally as needed. The potential use case of such an approach would be in testing whether a particular test could ever, reliably show non-equivalence. Take the MMSE as an example, there may be a true, large standardized effect difference between two groups, but if both of these groups are, on average, relatively unimpaired, then the considerable ceiling effect on the MMSE may preclude the observation of any such difference in raw scores. Thus, if one wanted to simply show that two groups are equivalent, then tests that selectively measure only one region of what may be thought of as a continuous underlying latent trait/scale/ability could be used to "hide" any raw score differences that may be occurring at the other end of that latent construct. Cognitive screening measures in particular are likely vulnerable to this issue as they measure primarily only the below average ranges of the latent "general cognitive ability" construct, making the detection of possible group differences in relatively unimpaired samples potentially impossible. The same is likely also true of naming measures like the BNT. Battery selection could thus be potentially informed by simulating situations in which there are true differences between groups and then testing whether raw score differences could be detected at various sample sizes using a particular test for which raw score averages and standard deviations are either known or easily calculated (e.g., via meta-norms, regression norms, etc.).
